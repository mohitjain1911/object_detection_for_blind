{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0375b-7d1e-411c-bce3-917eeb44c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install winspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c02753-4df9-44fe-9c7a-9cc3f99a590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3708361-d3b5-4ac3-9f7e-07399a719cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a856ef-7e9c-497e-ae66-769ff0d39a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb9da71-36d8-4dd9-92db-e8c53af7f152",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.src.saving.pickle_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m voice_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHKEY_LOCAL_MACHINE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSOFTWARE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMicrosoft\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpeech\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVoices\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTTS_MS_EN-US_ZIRA_11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m converter\u001b[38;5;241m.\u001b[39msetProperty(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoice\u001b[39m\u001b[38;5;124m'\u001b[39m, voice_id)\n\u001b[1;32m---> 18\u001b[0m render_model\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotesmodel.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     20\u001b[0m reference_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m46\u001b[39m\n\u001b[0;32m     21\u001b[0m person_diameter_inch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m17\u001b[39m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.saving.pickle_utils'"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "import pyttsx3\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import speech_recognition as sr\n",
    "import webbrowser\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "converter = pyttsx3.init()\n",
    "voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "converter.setProperty('voice', voice_id)\n",
    "\n",
    "\n",
    "render_model=pickle.load(open(\"notesmodel.pkl\", 'rb'))\n",
    "\n",
    "reference_distance = 46\n",
    "person_diameter_inch = 17 \n",
    "mobile_diameter_inch = 3.1 \n",
    "book_diameter_inch=7.5\n",
    "bottle_diameter_inch=2.9\n",
    "\n",
    "# Object detection configuration\n",
    "confidence_value = 0.35\n",
    "nms_value = 0.35\n",
    "type_fonts = cv.FONT_HERSHEY_SIMPLEX\n",
    "COLORS = [(227, 95, 54), (20, 95, 54), (20, 41, 54), (20, 41, 194), (74, 145, 194), (255, 0, 0),\n",
    "          (159, 112, 83), (90, 64, 112), (69, 50, 114), (111, 147, 114)]\n",
    "text_color = (107, 68, 202)\n",
    "rect_color = (0, 0, 0)\n",
    "\n",
    "# Load YOLO model\n",
    "yoloNet = cv.dnn.readNet('yolov4-tiny.weights', 'yolov4-tiny.cfg')\n",
    "yoloNet.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)\n",
    "yoloNet.setPreferableTarget(cv.dnn.DNN_TARGET_CUDA_FP16)\n",
    "class_names = []\n",
    "\n",
    "# Read class names from file\n",
    "with open(\"objects_in_universe.txt\", \"r\") as f:\n",
    "    class_names = [class_name.strip() for class_name in f.readlines()]\n",
    "\n",
    "# Initialize YOLO detection model\n",
    "model = cv.dnn_DetectionModel(yoloNet)\n",
    "model.setInputParams(size=(416, 416), scale=1/255, swapRB=True)\n",
    "\n",
    "def draw_object_info(image, classid, score, box):\n",
    "    color = COLORS[int(classid) % len(COLORS)]\n",
    "    adjust_score = (100 - score) / 800\n",
    "    unique_distort = abs(score - nms_value) / 100\n",
    "\n",
    "    label = \"%s : %f\" % (class_names[int(classid)], score)\n",
    "    label2 = \"Adjust_score:%f\" % (adjust_score)\n",
    "    label3 = \"Unique_distorted_score:%f\" % (unique_distort)\n",
    "\n",
    "    cv.rectangle(image, box, color, 2)\n",
    "    cv.putText(image, label, (box[0], box[1] - 14), type_fonts, 0.5, color, 2)\n",
    "    cv.putText(image, label2, (box[0], box[1] - 30), type_fonts, 0.5, color, 2)\n",
    "    cv.putText(image, label3, (box[0], box[1] - 46), type_fonts, 0.5, color, 2)\n",
    "\n",
    "def object_detector(image):\n",
    "    classes, scores, boxes = model.detect(image, confidence_value, nms_value)\n",
    "    data_list = []\n",
    "    for classid, score, box in zip(classes, scores, boxes):\n",
    "        draw_object_info(image, classid, score, box)\n",
    "        if classid in [0, 39, 67]:\n",
    "            data_list.append([class_names[classid], box[2], (box[0], box[1] - 2)])\n",
    "    return data_list\n",
    "\n",
    "def speakit(dist, entity):\n",
    "    engine.say(f'A {entity} is {dist} meter away')\n",
    "    engine.runAndWait()\n",
    "    converter.runAndWait()\n",
    "\n",
    "def focal_len_cmos_finder(reference_distance, diameter_inch, diameter_inch_in_rf):\n",
    "    return (diameter_inch_in_rf * reference_distance) / diameter_inch\n",
    "\n",
    "def dis_wrtreference_finder(focal, diameter_inch, width_in_frame):\n",
    "    return (diameter_inch * focal) / width_in_frame\n",
    "\n",
    "def process_reference_image(ref_image_path, ref_diameter_inch):\n",
    "    ref_image = cv.imread(ref_image_path)\n",
    "    ref_data = object_detector(ref_image)\n",
    "    print(ref_data)\n",
    "    ref_diameter_inch_in_rf = ref_data[0][1]\n",
    "    focal_length = focal_len_cmos_finder(reference_distance, ref_diameter_inch, ref_diameter_inch_in_rf)\n",
    "    return focal_length\n",
    "\n",
    "def recognize_speech():\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        print(\"Recognizing...\")\n",
    "        query = recognizer.recognize_google(audio)\n",
    "        print(f\"User said: {query}\")\n",
    "        return query.lower()\n",
    "    \n",
    "    except sr.UnknownValueError:\n",
    "        return \"\"\n",
    "    \n",
    "def speak(txt):  \n",
    "    engine.say(txt) \n",
    "    engine.runAndWait() \n",
    "    converter.runAndWait() \n",
    "\n",
    "# Process reference images\n",
    "focal_person = process_reference_image('ReferenceImages/person_ref.jpg', person_diameter_inch)\n",
    "focal_mobile = process_reference_image('ReferenceImages/mobile_ref.png', mobile_diameter_inch)\n",
    "focal_bottle = process_reference_image('ReferenceImages/bottle_ref.jpg', bottle_diameter_inch)\n",
    "\n",
    "# Capture video from camera\n",
    "cap = cv.VideoCapture(0)\n",
    "interval_sec = 10\n",
    "last_speak_time = time.time() - interval_sec\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    data = object_detector(frame)\n",
    "    user_command = recognize_speech()\n",
    "    spoken=user_command\n",
    "    if(spoken==\"check note\"):\n",
    "        currentframe = 0\n",
    "        while currentframe<3:\n",
    "            name = './data/frame' + str(currentframe) + '.jpg'\n",
    "            print ('Creating...' + name) \n",
    "            cv.imwrite(name, frame) \n",
    "            currentframe += 1\n",
    "            \n",
    "        img = image.load_img(\"./data/frame1.jpg\",target_size=(200,200))\n",
    "        x=image.img_to_array(img)\n",
    "        x=np.expand_dims(x,axis=0)\n",
    "        images=np.vstack([x])\n",
    "        val=render_model.predict(images)\n",
    "        max_value = np.argmax(val)\n",
    "        print(val)\n",
    "        if max_value==1:\n",
    "            valu=\"500Rs Note\"\n",
    "            print(valu)\n",
    "            speak(valu)\n",
    "        elif max_value==2:\n",
    "            valu=\"10Rs Note\"\n",
    "            print(valu)\n",
    "            speak(valu)\n",
    "        elif max_value==3:\n",
    "            valu=\"20Rs Note\"\n",
    "            print(valu)\n",
    "            speak(valu)\n",
    "        elif max_value==4:\n",
    "            valu=\"20Rs Note\"\n",
    "            print(valu)\n",
    "            speak(valu)\n",
    "        spoken=\"\"\n",
    " \n",
    "    for detected_obj in data:\n",
    "        if detected_obj[0] == 'person':\n",
    "            distance = dis_wrtreference_finder(focal_person, person_diameter_inch, detected_obj[1])\n",
    "            x, y = detected_obj[2]\n",
    "        elif detected_obj[0] == 'cell phone':\n",
    "            distance = dis_wrtreference_finder(focal_mobile, mobile_diameter_inch, detected_obj[1])\n",
    "            x, y = detected_obj[2]\n",
    "        elif detected_obj[0] == 'bottle':\n",
    "            distance = dis_wrtreference_finder(focal_mobile, mobile_diameter_inch, detected_obj[1])\n",
    "            x, y = detected_obj[2]\n",
    "\n",
    "        distance_text = round(distance * 0.0254, 2)\n",
    "        cv.rectangle(frame, (x, y - 3), (x + 150, y + 23), rect_color, -1)\n",
    "        cv.putText(frame, f'Distance: {distance_text}m', (x + 5, y + 13), type_fonts, 0.48, text_color, 2)\n",
    "        entity = detected_obj[0]\n",
    "\n",
    "        current_time = time.time()\n",
    "        if current_time - last_speak_time >= interval_sec:\n",
    "            speakit(distance_text, entity)\n",
    "            last_speak_time = current_time\n",
    "\n",
    "    cv.imshow('frame', frame)\n",
    "\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('x'):\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c2c2ed-faff-40cc-a758-db40c7983650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6512ca6-3f42-42d0-beee-b3bd94294bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=notesmodel.pkl. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(notesmodel.pkl, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# If you have access to the code that originally saved the model:\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotesmodel.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with the actual model loading method if necessary\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotesmodel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:199\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy H5 format files (`.h5` extension). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that the legacy SavedModel format is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported by `load_model()` in Keras 3. In \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder to reload a TensorFlow SavedModel as an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference-only layer in Keras 3, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`keras.layers.TFSMLayer(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, call_endpoint=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserving_default\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(note that your `call_endpoint` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    211\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File format not supported: filepath=notesmodel.pkl. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(notesmodel.pkl, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# If you have access to the code that originally saved the model:\n",
    "model = load_model(\"notesmodel.pkl\")  # Replace with the actual model loading method if necessary\n",
    "model.save(\"notesmodel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542289c-b356-4d80-b3b5-437a901f49b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
